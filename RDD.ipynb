{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark \n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/02 16:21:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://master:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>RDD test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f91c87b1250>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark:SparkSession = SparkSession.builder \\\n",
    "      .appName(\"RDD test\") \\\n",
    "      .getOrCreate()   \n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create RDD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create RDD by paralelize function\n",
    "data = [1,2,3,4,5,6,7,8,9,10,11,12]\n",
    "rdd=spark.sparkContext.parallelize(data)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Project Gutenberg’s',\n",
       " 'Alice’s Adventures in Wonderland',\n",
       " 'by Lewis Carroll',\n",
       " 'This eBook is for the use',\n",
       " 'of anyone anywhere',\n",
       " 'at no cost and with',\n",
       " 'Alice’s Adventures in Wonderland',\n",
       " 'by Lewis Carroll',\n",
       " 'This eBook is for the use',\n",
       " 'of anyone anywhere no no no']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create RDD from local sources\n",
    "rdd2 = spark.sparkContext.textFile(\"file:///home/iloveu/BK_bat_diet/20212/BigData/spark_test/test.txt\")\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test 1 so Transformation tren rdd2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FilteredRDD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 62:>                                                         (0 + 0) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alice’s Adventures in Wonderland', 'Alice’s Adventures in Wonderland']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "#Xem lai cac buoc lap trinh spark \n",
    "#b1: Tao RDD (nhu tren)\n",
    "#b2: Lazily transform thành các RDD trung gian\n",
    "#b3: cache() RDD để tái sử dụng\n",
    "#b4: Gọi các actions để thực thi tính toán song song và nhận về các kết quả\n",
    "\n",
    "#filter transformation\n",
    "filteredRDD = rdd2.filter(lambda line: \"Alice\" in line)\n",
    "print(\"FilteredRDD\")\n",
    "print(filteredRDD.collect())\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flatmapedRDD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 63:>                                                         (0 + 0) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Project', 'Gutenberg’s', 'Alice’s', 'Adventures', 'in', 'Wonderland', 'by', 'Lewis', 'Carroll', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'Alice’s', 'Adventures', 'in', 'Wonderland', 'by', 'Lewis', 'Carroll', 'This', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'no', 'no', 'no']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#FlatMap transformation\n",
    "flatmapedRDD = rdd2.flatMap(lambda line: line.split(\" \"))\n",
    "print(\"flatmapedRDD\")\n",
    "print(flatmapedRDD.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mapedRDD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 64:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Project', 1), ('Gutenberg’s', 1), ('Alice’s', 1), ('Adventures', 1), ('in', 1), ('Wonderland', 1), ('by', 1), ('Lewis', 1), ('Carroll', 1), ('This', 1), ('eBook', 1), ('is', 1), ('for', 1), ('the', 1), ('use', 1), ('of', 1), ('anyone', 1), ('anywhere', 1), ('at', 1), ('no', 1), ('cost', 1), ('and', 1), ('with', 1), ('Alice’s', 1), ('Adventures', 1), ('in', 1), ('Wonderland', 1), ('by', 1), ('Lewis', 1), ('Carroll', 1), ('This', 1), ('eBook', 1), ('is', 1), ('for', 1), ('the', 1), ('use', 1), ('of', 1), ('anyone', 1), ('anywhere', 1), ('no', 1), ('no', 1), ('no', 1)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#map transformation\n",
    "mapedRDD = flatmapedRDD.map(lambda line: (line,1))\n",
    "print(\"mapedRDD\")\n",
    "print(mapedRDD.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reducedByKeyRDD\n",
      "[('Project', 1), ('Gutenberg’s', 1), ('Alice’s', 2), ('in', 2), ('Lewis', 2), ('Carroll', 2), ('is', 2), ('use', 2), ('of', 2), ('anyone', 2), ('anywhere', 2), ('at', 1), ('no', 4), ('Adventures', 2), ('Wonderland', 2), ('by', 2), ('This', 2), ('eBook', 2), ('for', 2), ('the', 2), ('cost', 1), ('and', 1), ('with', 1)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#reduceByKey transformation - Den day la dem dc so lan xuat hien tu trong van ban\n",
    "reducedByKeyRDD = mapedRDD.reduceByKey(lambda a, b: a+ b)\n",
    "print(\"reducedByKeyRDD\")\n",
    "print(reducedByKeyRDD.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sortedByKeyRDD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 72:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'Project'), (1, 'Gutenberg’s'), (1, 'at'), (1, 'cost'), (1, 'and'), (1, 'with'), (2, 'Alice’s'), (2, 'in'), (2, 'Lewis'), (2, 'Carroll'), (2, 'is'), (2, 'use'), (2, 'of'), (2, 'anyone'), (2, 'anywhere'), (2, 'Adventures'), (2, 'Wonderland'), (2, 'by'), (2, 'This'), (2, 'eBook'), (2, 'for'), (2, 'the'), (4, 'no')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#sortByKey transformation - Sap xep cac tu theo tan xuat suat hien tang dan\n",
    "sortedByKeyRDD = reducedByKeyRDD.map(lambda x: (x[1], x[0])).sortByKey()\n",
    "print(\"sortedByKeyRDD\")\n",
    "print(sortedByKeyRDD.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test 1 so Action tren cac RDD vua tim dc**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count number of record: 23\n"
     ]
    }
   ],
   "source": [
    "# Action - count\n",
    "print(\"Count number of record: \"+str(sortedByKeyRDD.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Record : 1, Project\n"
     ]
    }
   ],
   "source": [
    "# Action - first\n",
    "firstRec = sortedByKeyRDD.first()\n",
    "print(\"First Record : \"+str(firstRec[0]) + \", \"+ firstRec[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Record : 4,no\n"
     ]
    }
   ],
   "source": [
    "# Action - max (theo key neu la dict)\n",
    "datMax = sortedByKeyRDD.max()\n",
    "print(\"Max Record : \"+str(datMax[0]) + \",\"+ datMax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42, 'Project')\n",
      "dataReduce Record : 42\n"
     ]
    }
   ],
   "source": [
    "# Action - reduce RDD thanh 1 record duy nhat, co the dung de dem so luong, o day la so tu cua vb\n",
    "totalWordCount = sortedByKeyRDD.reduce(lambda a,b: (a[0]+b[0],a[1]))\n",
    "print(totalWordCount)\n",
    "print(\"dataReduce Record : \"+str(totalWordCount[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "data3 Key:1, Value:Project\n",
      "data3 Key:1, Value:Gutenberg’s\n",
      "data3 Key:1, Value:at\n"
     ]
    }
   ],
   "source": [
    "# Action - take n first record of RDD\n",
    "data3 = sortedByKeyRDD.take(3)\n",
    "for f in data3:\n",
    "    print(\"data3 Key:\"+ str(f[0]) +\", Value:\"+f[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "Key:1, Value:Project\n",
      "Key:1, Value:Gutenberg’s\n",
      "Key:1, Value:at\n",
      "Key:1, Value:cost\n",
      "Key:1, Value:and\n",
      "Key:1, Value:with\n",
      "Key:2, Value:Alice’s\n",
      "Key:2, Value:in\n",
      "Key:2, Value:Lewis\n",
      "Key:2, Value:Carroll\n",
      "Key:2, Value:is\n",
      "Key:2, Value:use\n",
      "Key:2, Value:of\n",
      "Key:2, Value:anyone\n",
      "Key:2, Value:anywhere\n",
      "Key:2, Value:Adventures\n",
      "Key:2, Value:Wonderland\n",
      "Key:2, Value:by\n",
      "Key:2, Value:This\n",
      "Key:2, Value:eBook\n",
      "Key:2, Value:for\n",
      "Key:2, Value:the\n",
      "Key:4, Value:no\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Action - collect Dung nay h Chuyen data RDD thanh array(java) list(python) - Co the bi tran bo nho neu data qua lon\n",
    "data = sortedByKeyRDD.collect()\n",
    "print(type(data))\n",
    "for f in data:\n",
    "    print(\"Key:\"+ str(f[0]) +\", Value:\"+f[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Cuoi cung la action save file - No se tao ca cai folder luon \n",
    "sortedByKeyRDD.saveAsTextFile(\"file:///home/iloveu/BK_bat_diet/20212/BigData/spark_test/wordscount\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RDD Cache**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file:///home/iloveu/BK_bat_diet/20212/BigData/spark_test/test.txt MapPartitionsRDD[66] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PySpark RDD cache() method by default saves RDD computation to storage level `MEMORY_ONLY` \n",
    "#meaning it will store the data in the JVM heap as unserialized objects.\n",
    "cachedRDD = rdd2.cache()\n",
    "cachedRDD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b1fa783441936ab6be1890440a391a39396053c8c0014a2a5b73d7098a524eee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
